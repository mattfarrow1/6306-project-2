---
title: '6306: Case Study 2'
author: "Matt Farrow"
date: "11/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

# Project Description

Frito-Layâ„¢ (Client) has contracted DDSAnalytics to conduct an analysis of existing employee data in order to assist the Client with predicting employee turnover (attrition), with a goal of reducing or preventing attrition.

## Deliverables

Using R, conduct an analysis of the supplied data set with a goal of accomplishing the following:

1. Build a predictive model for attrition.

2. Identify the top three factors that contribute to attrition.

3. Identify any role-specific trends that may exist in the data.

4. Build a linear regression model for monthly income.

5. **OPTIONAL:** Create an RShiny app to assist with visualizing the results.

## Available Data

- `CaseStudy2-data.csv` is the data set with which to conduct the analysis.
- `CaseStudy2CompSet No Attrition.csv` is a data set of 300 observations that does not contain the attrition response variable and will be used to judge the efficacy of DDSAnalaytics' model. Client requires at least 60% sensitivity and specificity for the training and validation sets.
- `CaseStudy2CompSet No Salary.csv` is a data set of 300 observations that does not contain the monthly income variable. Client requires the predictive model attain an RMSE < $3,000 for the training and validation sets.

## Model Requirements

The predictive model for classifying attrition should be built using either K-Nearest Neighbors or Naive Bayes. Additional models may be used for comparison and to fulfill the sensitivity/specificity requirement. 

The regression must include linear regression, but additional models may be used for comparison and to fulfill the salary prediction competition.

# Setup

## Load Libraries

```{r}
library(tidyverse)  # general data processing & plotting
library(here)       # relative location references
library(janitor)    # data cleanup tools
library(naniar)     # dealing with missing values
library(caret)      # misc functions for training and plotting classification and regression models
# library(tidymodels)
library(GGally)     # for ggpairs
library(MASS)       # for LDA/QDA
library(dplyr)      # to get `select` back natively
library(inspectdf)  # data inspection reports
library(hrbrthemes) # preferred themes for ggplot
library(e1071)      # Naive Bayes
library(scales)     # formatting axes in ggplot
```

## Load Data

```{r}
# Load Case Study 2 data
df <- read_csv(here("data - raw", "CaseStudy2-data.csv"))

# Load comp data without salary
df_comp_no_sal <- readxl::read_excel(here("data - raw", "CaseStudy2CompSet No Salary.xlsx"))

# Load comp data without attrition
df_comp_no_att <- read_csv(here("data - raw", "CaseStudy2CompSet No Attrition.csv"))
```

## Initial Cleanup & Examination

```{r}
# Clean the column names of each of the three data sets
df             <- clean_names(df)
df_comp_no_sal <- clean_names(df_comp_no_sal)
df_comp_no_att <- clean_names(df_comp_no_att)

head(df, 10)
dim(df)
glimpse(df)
summary(df)
skimr::skim(df)
visdat::vis_dat(df)
# DataExplorer::create_report(df)
# Hmisc::describe(df)
# psych::describe(df)

# Reports from inspectdf
inspect_cat(df)
inspect_cor(df)
inspect_types(df)
inspect_num(df)
inspect_imb(df)
```

## Observations from DataExplorer::create_report

- No missing values
- `id` column isn't useful and can be dropped
- `over18` is exclusively "Y" and can be dropped
- There is only one unique value of `employee_count` - the number 1
- There is only one unique value of `standard_hours` - the number 80
- `age` is the only variable that generally follows a standard deviation according to the QQ plots
- There appear to be some fairly defined clusters in the rate and income columns (`daily_rate`, `hourly_rate`, `monthly_income`, `monthly_rate`). In addition, all of the rate columns seem to follow similar trends. It may be worth exploring whether all are needed.
- A number of numeric columns appear that they may actually be numerically-coded categorical variables and may need to be re-coded:
  - `education`
  - `environment_satisfaction`
  - `job_involvement`
  - `job_level`
  - `job_satisfaction`
  - `performance_rating`
  - `relationshup_satisfaction`
  - `stock_option_level`
  - `work_life_balance`
- Overall attrition is 16.1%

# EDA

```{r}
# Remind myself what the column names are
colnames(df)

# Run correlations and save output as images to be used in the appendix.
ggcorr(
  df,
  label = TRUE,
  label_alpha = TRUE,
  label_size = 3,
  layout.exp = 2,
  cex = 3.5,
  hjust = 1
)
# ggsave(here::here("images", "correlation 1.png"))

glimpse(df)
```

```{r}
# Convert all character variables to factors
df_clean <- df %>% 
  mutate(across(where(is_character),as_factor))
```

It appears that there are a number of variables that are coded as numeric values, but are more likely numeric codes of categorical variables. We'll re-code those as factors.

```{r}
df_clean <- df_clean %>%
  mutate(across(
    c(
      education,
      environment_satisfaction,
      job_involvement,
      job_level,
      job_satisfaction,
      performance_rating,
      relationship_satisfaction,
      stock_option_level,
      work_life_balance
    ),
    as_factor
  ))
```

In order to focus our data set, we'll remove variables that appear to not be useful.

```{r}
df_clean <- df_clean %>% 
  dplyr::select(-c(id,
                   employee_count,
                   over18,
                   standard_hours,
                   employee_number))
```

## Natural Attrition

Having whittled down the data set, let's look more closely at the relationships between variables and attrition. First, we'll see if there is any sort of natural attrition due to employees retiring or aging out of the workforce that should be addressed.

```{r}
# Age
df_clean %>% 
  ggplot(aes(age, color = attrition)) +
  geom_boxplot() +
  labs(title = "Relationship Between:",
       subtitle = "Age & Attrition",
       x = "Age",
       color = "Attrition") +
  theme_ipsum()

# Total Working Years
df_clean %>% 
  ggplot(aes(total_working_years, color = attrition)) +
  geom_boxplot() +
  labs(title = "Relationship Between:",
       subtitle = "Total Working Years & Attrition",
       x = "Total Working Years",
       color = "Attrition") +
  theme_ipsum()

# Years at Company
df_clean %>% 
  ggplot(aes(years_at_company, color = attrition)) +
  geom_boxplot() +
  labs(title = "Relationship Between:",
       subtitle = "Years at Company & Attrition",
       x = "Years at Company",
       color = "Attrition") +
  theme_ipsum()
```

It certainly seems like people who are likely at the tail end of their careers are leaving the company naturally and don't need to be included in this model. 25-30 total working years seems reasonable. Let's see if there's a big difference in numbers:

```{r}
df_clean %>%
  filter(total_working_years >= 25) %>% 
  group_by(total_working_years) %>% 
  count() %>% 
  pivot_wider(names_from = total_working_years,
              values_from = n) %>% 
  mutate(twenty_five_plus = sum(1:14, na.rm = TRUE),
         thirty_plus = sum(6:14, na.rm = TRUE)) %>% 
  dplyr::select(15:16)
```

There is a 15-person (1.72%) difference between filtering total working years at 25 years vs. 30 years. To be safe, we'll exclude anyone who has worked more than 30 years.

```{r}
df_clean <- df_clean %>% 
  filter(total_working_years < 31)
```

Look at the correlation plot again after the work we've done so far.

```{r}
ggcorr(
  df_clean,
  label = TRUE,
  label_alpha = TRUE,
  label_size = 3,
  layout.exp = 2,
  cex = 3.5,
  hjust = 1
)
```

## Explore Attrition by Variable

### Job Level

```{r}
df_clean %>% 
  group_by(job_level, attrition) %>% 
  count() %>% 
  ggplot(aes(attrition, n, fill = attrition)) +
  geom_col() +
  facet_wrap(~ job_level, scales = "free_y") +
  labs(title = "Attrition by Job Level",
       x = "",
       y = "Count",
       fill = "Attrition") +
  theme_ipsum()
```

If we presume that job level increases sequentially the longer he or she is in the workforce, newer workers (1) appear much more susceptible to attrition.

### Job Role

```{r}
df_clean %>% 
  group_by(job_role, attrition) %>% 
  count() %>% 
  ggplot(aes(attrition, n, fill = attrition)) +
  geom_col() +
  facet_wrap(~ job_role, scales = "free", ncol = 2) +
  labs(title = "Attrition by Job Role",
       x = "",
       y = "",
       fill = "Attrition") +
  theme_ipsum() +
  theme(axis.text.x = element_blank(),
        legend.position = "none")

df_clean %>% 
  group_by(job_role, attrition) %>% 
  count() %>% 
  pivot_wider(names_from = attrition,
              values_from = n) %>% 
  mutate(total = No + Yes,
         pct = Yes / total)
```

Based on the charts, sales representatives are split almost 50/50 on whether they'll leave the company. Other levels that also appear to have significant rates of attrition include research scientists, human resources, and possibly laboratory technicians.

### Gender

```{r}
df_clean %>% 
  group_by(gender, attrition) %>% 
  count() %>% 
  pivot_wider(names_from = attrition,
              values_from = n) %>% 
  mutate(Total = No + Yes,
         Proportion = Yes / Total) %>% 
  ggplot(aes(gender, Proportion)) +
  geom_col(fill = "steelblue") +
  scale_y_continuous(labels = percent) +
  labs(title = "Proportion of Each Gender that\nSuffers from Attrition",
       x = "",
       y = "Proportion") +
  theme_ipsum()
```

It appears that males tend to suffer from attrition at slightly higher rates than females.

### Gender & Age

```{r}
df_clean %>% 
  group_by(gender, attrition, age) %>% 
  count(age) %>% 
    ggplot(aes(age, n, fill = attrition)) +
  geom_col(alpha = 0.6) +
  facet_wrap(~ gender, scales = "free_y") +
  labs(title = "Attrition by Age & Gender",
       x = "",
       y = "Count",
       fill = "Attrition") +
  theme_ipsum()
```

Looking at the two sets of histograms, it appears that both male and female attrition by age follow very similar distributions, however a couple of interesting observations can be seen:

- More females aged 50-60 appear to remain in the workforce, while more males in the same age bracket appear to leave the company.
- The histograms show slight right-skew distributions with a median around 35 years old.

### Monthly Income

```{r}
df_clean %>% 
  ggplot(aes(monthly_income, fill = attrition)) +
  geom_histogram(alpha = 0.5) +
  scale_x_continuous(labels = dollar) +
  labs(title = "Attrition by Monthly Income",
       x = "Monthly Income",
       y = "",
       fill = "Attrition") +
  theme_ipsum()
```

As we'd expect, the distribution of monthly income is right-skewed with most employees appearing to land in the $2,500-$5,000 per month range. But is it statistically significant?

```{r}
# NULL & Alternative

# H_0: the mean monthly income for those who left is equal to those who did not

# H_A: the mean monthly income is not equal between the two groups

# Run t.test
t.test(monthly_income ~ attrition, 
       data = df_clean,
       alternative = "two.sided",
       mu = 0,
       conf.level = 0.95)
```

Based on a Welch's Two-Sample t-test, we reject our NULL hypothesis that the mean monthly income between those who left and those who stayed is equal (p-value = 2.929e-07). A 95% confidence interval of monthly income is between $1,141.94 and $2,498.72. 

Employee attrition is concentrated within monthly incomes less than $5,000/month which makes sense. Employees who need or desire a higher salary, but are unable to achieve it within the company, will look for employment elsewhere with a higher salary.

It's interesting to note the stair-step pattern that starts to appear around $12,500/month and peaks around $14,000/month. The trend then repeats twice more. 

It's possible that the attrition from employees making around $20,000 a month is voluntary attrition from people who are either looking to retire or take on new challenges at other companies.

### Age & Years at Company

```{r}
df_clean %>% 
  ggplot(aes(age, years_at_company, color = attrition)) +
  geom_jitter(alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(title = "Attrition by Age & Years at Company",
       x = "Age",
       y = "Years at Company",
       fill = "Attrition") +
  theme_ipsum()
```

### Age & Total Working Years

```{r}
df_clean %>% 
  ggplot(aes(age, total_working_years, color = attrition)) +
  geom_jitter(alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(title = "Attrition by Age & Total Working Years",
       x = "Age",
       y = "Total Working Years",
       fill = "Attrition") +
  theme_ipsum()
```

Both of these charts show in slightly more detail trends that we've already observed in previous histograms. Let's run t.tests to see if there is a statistically significant difference.

```{r}
# Run t.test
t.test(age ~ attrition, 
       data = df_clean,
       alternative = "two.sided",
       mu = 0,
       conf.level = 0.95)

# Run t.test
t.test(years_at_company ~ attrition, 
       data = df_clean,
       alternative = "two.sided",
       mu = 0,
       conf.level = 0.95)

# Run t.test
t.test(total_working_years ~ attrition, 
       data = df_clean,
       alternative = "two.sided",
       mu = 0,
       conf.level = 0.95)
```

### Distance from Home

```{r}
df_clean %>% 
  ggplot(aes(distance_from_home, fill = attrition)) +
  geom_histogram(alpha = 0.5) +
  labs(title = "Attrition by Distance from Home",
       x = "Distance (miles)",
       y = "",
       fill = "Attrition") +
  theme_ipsum()
```

The trends for attrition vs. no-attrition seem to remain fairly consistent with one another as employees have longer and longer commutes.

### Overtime

```{r}
df_clean %>% 
  group_by(over_time, attrition) %>% 
  count() %>% 
  ggplot(aes(over_time, n, fill = attrition)) +
  geom_col(alpha = 0.6) +
  labs(title = "Attrition by Overtime Eligibility",
       x = "Overtime Eligible?",
       y = "",
       fill = "Attrition") +
  theme_ipsum() +
  theme(legend.position = "none")

df_clean %>% 
  group_by(over_time, attrition) %>% 
  count() %>% 
  pivot_wider(names_from = attrition,
              values_from = n) %>% 
  mutate(Total = No + Yes,
         PCT = Yes / Total)
```

It appears that attrition is a much more significant problem for employees who are overtime eligible. This lines up with what we know about the American workforce and our previous examination of monthly income. Lower paid workers tend to be those that are overtime eligible and both groups have higher rates of attrition.

### Environment Satisfaction

```{r}
df_clean %>% 
  group_by(environment_satisfaction, attrition) %>% 
  count() %>% 
  ggplot(aes(environment_satisfaction, n, fill = attrition)) +
  geom_col(alpha = 0.6) +
  labs(title = "Attrition by Environment Satisfaction",
       x = "Environment Satisfaction",
       y = "",
       fill = "Attrition") +
  theme_ipsum() +
  theme(legend.position = "none")

df_clean %>% 
  group_by(environment_satisfaction, attrition) %>% 
  count() %>% 
  pivot_wider(names_from = attrition,
              values_from = n) %>% 
  mutate(Total = No + Yes,
         PCT = Yes / Total)
```

There is a 10% level of attrition with employees who list their environment satisfaction as a 1.

### Work Life Balance

```{r}
df_clean %>% 
  group_by(work_life_balance, attrition) %>% 
  count() %>% 
  ggplot(aes(work_life_balance, n, fill = attrition)) +
  geom_col(alpha = 0.6) +
  labs(title = "Attrition by Work/Life Balance",
       x = "Work/Life Balance",
       y = "",
       fill = "Attrition") +
  theme_ipsum() +
  theme(legend.position = "none")

df_clean %>% 
  group_by(work_life_balance, attrition) %>% 
  count() %>% 
  pivot_wider(names_from = attrition,
              values_from = n) %>% 
  mutate(Total = No + Yes,
         PCT = Yes / Total)
```

Although they were the smallest population (48 out of 870), employees who rated their work/life balance as a 1 were 20% more likely to suffer from attrition.

# Prediction Models

## Test/Train Splits

### Full Data

```{r}
# Set seed
set.seed(123)

# Create test/train data sets of full data
inTraining <-
  createDataPartition(df_clean$attrition, p = .75, list = FALSE)
train_full <- df_clean[ inTraining,]
test_full  <- df_clean[-inTraining,]
```

### Full Data, down-sampled

```{r}
# Down-sample the data to get an equal number of yes and no responses
df_clean_ds <- downSample(x = df_clean[, c(1, 3:31)],
                          y = df_clean$attrition)

# Rename Class to attrition
df_clean_ds <- df_clean_ds %>% 
  rename(attrition = Class)

# Create test/train data sets of full, down-sampled data
inTraining <-
  createDataPartition(df_clean_ds$attrition, p = .75, list = FALSE)
train_full_ds <- df_clean_ds[inTraining, ]
test_full_ds  <- df_clean_ds[-inTraining, ]
```

### Numeric-Only (for KNN)

```{r}
# Set seed
set.seed(123)

# Create a KNN data set that contains only attrition and numeric variables
df_clean_knn <- df_clean %>% 
  dplyr::select(c(1, 4, 6, 11, 17:19, 21, 25:26, 28:31, 2))

# Create test/train data sets of full data
inTraining <-
  createDataPartition(df_clean_knn$attrition, p = .75, list = FALSE)
train_knn <- df_clean_knn[ inTraining,]
test_knn  <- df_clean_knn[-inTraining,]
```

### Numeric-Only (for KNN), pre-processed

```{r}
# Estimate pre-processing parameters
preproc_parameter <- train_knn %>%
  preProcess(method = c("scale"))

# Transform the data using the estimated parameters
train_knn_transform <- preproc_parameter %>% predict(train_knn)
test_knn_transform <- preproc_parameter %>% predict(test_knn)

# Double-check the number of yes/no responses
train_knn_transform %>%
  count(attrition)
```

### Numeric-Only (for KNN), down-sampled

```{r}
# Down-sample the data to get an equal number of yes and no responses
df_clean_knn_ds <- downSample(x = df_clean_knn[, 1:14],
                              y = df_clean_knn$attrition)

# Rename Class to attrition
df_clean_knn_ds <- df_clean_knn_ds %>% 
  rename(attrition = Class)

# Create test/train data sets of numeric, down-sampled data
inTraining <-
  createDataPartition(df_clean_knn_ds$attrition, p = .75, list = FALSE)
train_knn_ds <- df_clean_knn_ds[inTraining, ]
test_knn_ds  <- df_clean_knn_ds[-inTraining, ]

# Double-check the number of yes/no responses
train_knn_ds %>%
  count(attrition)
```

## KNN

```{r}
train_set <- train_knn_transform
test_set  <- test_knn_transform

# Create control parameters for train
knn_control <- trainControl(
  method = "repeatedcv",
  repeats = 10,
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Create KNN model
knn_model <- train(
  attrition ~ .,
  data = train_set,
  method = "knn",
  trControl = knn_control,
  tuneGrid = expand.grid(.k = seq(2, 20, by = 2)),
  metric = "ROC"
)

# View results of KNN
knn_model
```

Ultimately the performance of the preprocessed KNN model is the best performing, achieving a ROC of 0.64 at k = 20). Because the KNN model is based only on the numeric values, we'd expect it to perform less well than other models.

## Naive Bayes

```{r}
# # Create Naive Bayes model using naivebayes packages to generate plots
# 
# # Create Naive Bayes model
# nb_model <- naivebayes::naive_bayes(attrition ~ ., 
#                                     data = train_full,
#                                     laplace = 0.5)
# 
# # View results of Naive Bayes
# nb_model
# 
# # View summary of Naive Bayes
# summary(nb_model)
# 
# # Plot Naive Bayes results
# plot(nb_model)
# 
# # Run predictions on the testing data
# nb_predict <- predict(nb_model,
#                       test_full,
#                       type = "raw")
# 
# # Create confusion matrix
# confusionMatrix(table(predict(nb_model, 
#                               test_full), 
#                       test_full$attrition))
```

```{r}
# Create Naive Bayes model
nb_model <- naiveBayes(attrition ~ .,
                       data = train_full)

# View results of Naive Bayes
nb_model

# View summary of Naive Bayes
summary(nb_model)

# Run predictions on the testing data
nb_predict <- predict(nb_model,
                      test_full,
                      type = "raw")

# Create confusion matrix
confusionMatrix(table(predict(nb_model, 
                              test_full), 
                      test_full$attrition))
```

The Naive Bayes model performs much better than the KNN model. Accuracy is up to 76.96%, sensitivity is 78.02%, and specificity is 71.43%.

Does changing the cutoff value improve performance at all?

```{r}
nb_cutoff <- factor(if_else(nb_predict[, 2] > 0.4, 
                            "Yes", 
                            "No"))

confusionMatrix(nb_cutoff, test_full$attrition)
```

With a cutoff value of 0.4, accuracy drops to 74.65%, sensitivity drops to 74.73%%, but specificity rises to 74.29%. Our first Naive Bayes model is the best performing so far.

## Random Forest

```{r}
# Create random forest model

# Define cross-validation attempts
rf_grid <- expand.grid(mtry = c(10, 50, 100, 500, 1000))

# Create random forest control parameters
rf_control <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)

# Create random forest model
rf_model <- train(
  attrition ~ .,
  data = train_full,
  method = "rf",
  trControl = rf_control,
  tuneGrid = rf_grid,
  metric = "ROC"
)

rf_model
varImp(rf_model)
```

## Naive Bayes (part 2)

What happens if we re-run our Naive Bayes model with the top 8 variables from the random forest model?

```{r}
# Create Naive Bayes model
nb_model <- naiveBayes(attrition ~
                         age +
                         monthly_income +
                         over_time +
                         total_working_years +
                         daily_rate +
                         distance_from_home +
                         years_at_company +
                         percent_salary_hike,
                       data = train_full)

# View results of Naive Bayes
nb_model

# View summary of Naive Bayes
summary(nb_model)

# Run predictions on the testing data
nb_predict <- predict(nb_model,
                      test_full,
                      type = "raw")

# Create confusion matrix
confusionMatrix(table(predict(nb_model, 
                              test_full), 
                      test_full$attrition))
```

Using the top predictors from the random forest model in our naive bayes model, we get an accuracy of 81.57%, a specificity of 89.01%, but a specificity of 42.86%.

## Linear Regression Models

### All Variables

```{r}
# Create linear model
lm_model <- lm(monthly_income ~ .,
               data = train_full)

# View model summary
summary(lm_model)

# Review residual plots
plot(lm_model)
```

The linear regression model using all of the variables returns an Adjusted R-Squared value of 0.9417. The QQ plot indicates `monthly_income` is not normally distributed (which lines up with our EDA). We'll see if a log transformation helps.

### Log Transformation

```{r}
# Create linear model
lm_model <- lm(log(monthly_income) ~ .,
               data = train_full)

# View model summary
summary(lm_model)

# Review residual plots
plot(lm_model)
```

The residuals with the log transformation look much more normally distributed, although Adjusted R-Squared drops to 0.8814. We'll proceed with this transformation.

### Check Predictions

```{r}
# Run predictions on test set and check model performance
lm_predict <- predict(lm_model, newdata = test_full)
postResample(exp(lm_predict), test_full$monthly_income)
```
With the log transformation, our model produces an RMSE of $1,089.94 and an R-Squared of 0.9407.

# Deliverables

## Attrition Classifier

### Match Data to Model Data

```{r}
# Convert all character variables to factors
df_comp_no_att_clean <- df_comp_no_att %>% 
  mutate(across(where(is_character),as_factor))

# Re-code certain numeric values as factors
df_comp_no_att_clean <- df_comp_no_att_clean %>%
  mutate(across(
    c(
      education,
      environment_satisfaction,
      job_involvement,
      job_level,
      job_satisfaction,
      performance_rating,
      relationship_satisfaction,
      stock_option_level,
      work_life_balance
    ),
    as_factor
  ))

# Remove unhelpful variables
df_comp_no_att_clean <- df_comp_no_att_clean %>% 
  dplyr::select(-c(employee_count,
                   over18,
                   standard_hours,
                   employee_number))
```

### Predictions

```{r}
# Apply prediction to data set
predict_attrition <- predict(knn_model,
                             newdata = df_comp_no_att_clean[, 2:31])

# Merge predictions back into data set
df_comp_no_att_clean_done <- df_comp_no_att_clean %>% 
  mutate(attrition = predict_attrition)

# Keep only the ID and Prediction columns
df_comp_no_att_clean_done <- df_comp_no_att_clean_done %>% 
  dplyr::select(id, attrition)

# Save data
write_csv(df_comp_no_att_clean_done, here("data - output", "Farrow_Attrition_Predictions.csv"))
```

## Salary Predictions

### Match Data to Model Data

```{r}
# Convert all character variables to factors
df_comp_no_sal_clean <- df_comp_no_sal %>% 
  mutate(across(where(is_character),as_factor))

# Re-code certain numeric values as factors
df_comp_no_sal_clean <- df_comp_no_sal_clean %>%
  mutate(across(
    c(
      education,
      environment_satisfaction,
      job_involvement,
      job_level,
      job_satisfaction,
      performance_rating,
      relationship_satisfaction,
      stock_option_level,
      work_life_balance
    ),
    as_factor
  ))

# Remove unhelpful variables
df_comp_no_sal_clean <- df_comp_no_sal_clean %>% 
  dplyr::select(-c(employee_count,
                   over18,
                   standard_hours,
                   employee_number))
```

### Predictions

```{r}
# Apply prediction to data set
predict_salary <- predict(lm_model,
                             newdata = df_comp_no_sal_clean[, 2:31])

# Merge predictions back into data set
df_comp_no_sal_clean_done <- df_comp_no_sal_clean %>% 
  mutate(monthly_income = exp(predict_salary))

# Keep only the ID and Prediction columns
df_comp_no_sal_clean_done <- df_comp_no_sal_clean_done %>% 
  dplyr::select(id, monthly_income)

# Save data
write_csv(df_comp_no_sal_clean_done, here("data - output", "Farrow_Salary_Predictions.csv"))
```